{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pyspark.sql import SQLContext\n",
    "import json\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.jsonFile(\"./spark_tutorial_article.json\")\n",
    "\n",
    "gf = df.map(lambda x : (x[2],x[5],x[12]))\n",
    "\n",
    "print type(gf)\n",
    "#spark.read.json(sc.wholeTextFiles('./spark_tutorial_article.json').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc.textFile(\"./spark_tutorial_article.json\").map(json.loads).take(1)[0][u'author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用BeautifulSoup擷取內容，並套用Jieba斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## getContent: for input aritcle, get it own word set via jieba.cut()\n",
    "def getContent(x):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(x)\n",
    "    text = soup.getText().replace('\\n','').replace('\\r','').replace(' ','').replace('\\t','')\n",
    "    import jieba\n",
    "    r = list()\n",
    "    for term in jieba.cut(text):\n",
    "        if len(term) > 1 and checkword(term): r.append(term)\n",
    "    return r\n",
    "\n",
    "def checkword(x):\n",
    "    return all(u'\\u4e00' <= c <= u'\\u9fff' for c in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_token = gf.map(lambda x: (x[0], getContent(x[1]), x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check text_token\n",
    "#text_token.first()\n",
    "#text_token.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算每篇文章的TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_tf(tokens):\n",
    "    d = {}\n",
    "    for word in tokens:\n",
    "        if not word in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    for word in d:\n",
    "        d[word] = float(d[word])/len(tokens)\n",
    "    return d\n",
    "\n",
    "text_token_tf = text_token.map(lambda x: cal_tf(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check text_token_tf\n",
    "#text_token_tf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cal_idf(docs):\n",
    "    N = docs.count()\n",
    "    uniqueTokens = docs.map(lambda x : list(set(x[1])))\n",
    "    token_sum_tuples = uniqueTokens.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y)\n",
    "    return token_sum_tuples.map(lambda x : (x[0], float(N)/x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(tokens, idfs):\n",
    "    tfs = cal_tf(tokens)\n",
    "    for tk in tfs:\n",
    "        tfs[tk] = tfs[tk]*idfs[tk]\n",
    "    tfidf_Dict = tfs\n",
    "    return tfidf_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美味食記\n"
     ]
    }
   ],
   "source": [
    "doc_idfs = cal_idf(text_token)\n",
    "\n",
    "doc_c = doc_idfs.collectAsMap()  #my idf dict\n",
    "\n",
    "text_tfidf =  TFIDF(text_token.collect()[0][1], doc_c)\n",
    "\n",
    "print text_token.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check text_tfidf\n",
    "#text_tfidf\n",
    "#text_token.collect()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    dotsum = 0\n",
    "    for tk in a:\n",
    "        if tk in b:\n",
    "            dotsum += a[tk]*b[tk]\n",
    "    return dotsum\n",
    "\n",
    "def norm(a):\n",
    "    return math.sqrt(dotprod(a,a))\n",
    "\n",
    "def cossim(a, b):\n",
    "    return dotprod(a,b)/(norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    w1 = tfidf(string1, idfsDictionary)\n",
    "    w2 = tfidf(string2, idfsDictionary)\n",
    "    return cossim(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Rule One - top words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showTopWord(link):\n",
    "    tokens = text_token.filter(lambda x: x[2] == link).collect()[0][1]\n",
    "    tokens_weights = TFIDF(tokens, doc_c)\n",
    "    print type(tokens_weights)\n",
    "    tokens_weights_sorted = sorted(tokens_weights, key=tokens_weights.get, reverse=True)\n",
    "    for index in range(0,9):\n",
    "        print tokens_weights_sorted[index], tokens_weights[tokens_weights_sorted[index]]\n",
    "    print tokens_weights_sorted[:14]\n",
    "    return tokens_weights_sorted[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = u'http://lovecc6.pixnet.net/blog/post/73513867'\n",
    "#showTopWord(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = text_token.map(lambda x : x[2])\n",
    "\n",
    "#top_word_list = [showTopWord(i) for i in urls]\n",
    "#top_word_list = urls.map(lambda x: showTopWord(x))\n",
    "#top_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rule Two - Query in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_input = [u'海鮮', u'魷魚', u'甜點']\n",
    "\n",
    "def check_in(query, text):\n",
    "    count = 0\n",
    "    for q in query:\n",
    "        if q in text:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def query_points(query):\n",
    "    query_points_table = text_token.map(lambda x : check_in(query, x[1]))\n",
    "    return query_points_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_pts = query_points(query_input).collect()\n",
    "\n",
    "len(query_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule 3 - Term Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_weights(tokens):\n",
    "    d = {}\n",
    "    for word in tokens:\n",
    "        if not word in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def term_points(query, point_dict):\n",
    "    points = 0\n",
    "    for i in query:\n",
    "        if i in point_dict:\n",
    "            points += point_dict[i]\n",
    "                \n",
    "    return points\n",
    "\n",
    "tf_list = text_token.map(lambda x : term_weights(x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_pts = [term_points(query_input, i) for i in tf_list]\n",
    "len(term_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_points(term_weight_pts, query_pts):\n",
    "#    tw_dict = text_token.map(lambda x: term_weights(x[1])).collect()\n",
    "#    doc_point = text_token.map(lambda x : (((term_points(query_input, tw_dict))*(check_in(query_input, x[1])) , x[2])))\n",
    "    doc_point = [i*j for i,j in zip(term_weight_pts, query_pts)]\n",
    "    \n",
    "    return doc_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_list = text_token.map(lambda x : (x[2]))\n",
    "\n",
    "total_pts = zip(doc_points(term_pts, query_pts) , url_list.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print type(total_pts)\n",
    "total_pts_sort = sorted(total_pts, reverse=True)\n",
    "#total_pts_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(58, u'http://wonderfood.pixnet.net/blog/post/191227599'),\n",
       " (51, u'http://dasoe.pixnet.net/blog/post/23979807'),\n",
       " (51, u'http://dasoe.pixnet.net/blog/post/23979807'),\n",
       " (46,\n",
       "  u'http://kshj168997.pixnet.net/blog/post/114162104-%e5%90%9b%e5%93%81%e9%85%92%e5%ba%97-%e9%9b%b2%e8%bb%92-la-rotisserie-%e6%b5%aa%e6%bc%ab%e6%b0%a3%e6%b0%9b%e4%b8%8b%e5%8d%88%e8%8c%b6%e5%90%83'),\n",
       " (45, u'http://wonderfood.pixnet.net/blog/post/197652978'),\n",
       " (40, u'http://hsheena.pixnet.net/blog/post/31067335'),\n",
       " (40,\n",
       "  u'http://drugs.pixnet.net/blog/post/38322439-%e9%a6%99%e6%a0%bc%e9%87%8c%e6%8b%89%e9%81%a0%e6%9d%b1cafe%e8%87%aa%e5%8a%a9%e9%a4%90%e5%bb%b3'),\n",
       " (40,\n",
       "  u'http://drugs.pixnet.net/blog/post/38322439-%e9%a6%99%e6%a0%bc%e9%87%8c%e6%8b%89%e9%81%a0%e6%9d%b1cafe%e8%87%aa%e5%8a%a9%e9%a4%90%e5%bb%b3'),\n",
       " (38, u'http://dm0520.com/blog/post/33442603'),\n",
       " (33, u'http://smilezi.pixnet.net/blog/post/101210549')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pts_sort[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
